# -*- coding: utf-8 -*-
"""wikipedia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HThIFcPjMe7yBxqo36qavOm4_UNJQ78f
"""

import networkx as nx
import matplotlib.pyplot as plt
import nltk
nltk.download('all-nltk')

pip install wikipedia

import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
import wikipedia
from nltk.stem import PorterStemmer
from gensim.summarization import keywords
stemmer = PorterStemmer()

def bag_of_words(text): 
  #cleaning the texts and creating the corpus             
  ps = PorterStemmer()
  wordnet=WordNetLemmatizer()
  sentences = nltk.word_tokenize(text)
  corpus = []
  for i in range(len(sentences)):
      review = re.sub('[^a-zA-Z]', ' ', sentences[i])
      review = review.lower()
      review = review.split()
      review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
      review = ' '.join(review)
      corpus.append(review)    
  # Creating the Tf-Idf model
  from sklearn.feature_extraction.text import TfidfVectorizer
  cv = TfidfVectorizer()
  X = cv.fit_transform(corpus).toarray()
  return X

#only for testing, nothing else
tes = '''Challenges in natural language processing frequently involve
... speech recognition, natural language understanding, natural language
... generation (frequently from formal, machine-readable logical forms),
... connecting language and machine perception, dialog systems, or some
... combination thereof.'''
lost = list(keywords(tes).split("\n"))
print(lost)

#function to extract feature from wikipedia pages of each title 
def features(title):
  wiki = wikipedia.page(title)
  text = wiki.content
  l = list(keywords(text).split("\n"))
  #cleaning the text
  stop = set(stopwords.words('english'))
  stems = [stemmer.stem(i) for i in l if i not in stop]
  key_words = set(stems)
  x = bag_of_words(text)
  return x,key_words

#code to scrap all wikipedia articles of Mathematics topics and storing its links in storage dictionary
from bs4 import BeautifulSoup
import requests
import random
count = 0
#queue is the list of all the links that are the values of the storage keys
queue=[]
#creating a storage dictionary key values as topics and their values as the list of links under that topic 
storage = dict()
feature_vec = dict()
key_w = dict()

# https://www.freecodecamp.org/news/scraping-wikipedia-articles-with-python/
def scrapeWikiArticle(url):
  links_to_scrape = []
  try:
    response = requests.get( url=url,)
    soup = BeautifulSoup(response.content, 'html.parser')
    title = soup.find(id="firstHeading")
    allLinks = soup.find(id="bodyContent").find_all("a")
    #extracting features and keywords from text
    featur,key_word = features(title)
    h=url.split('.org')
    feature_vec[h[1]] = featur
    key_w[h[1]]=key_word
    #appending only the wiki links to the queue
    for link in allLinks:
      try:
         if link['href'].find("/wiki/") == -1: 
           continue
         if "https"  in link['href']:
             continue
         else:
            links_to_scrape.append(link['href'])
            queue.append(link['href'])
      except:
           pass
    
  except:
     pass 
  h=url.split('.org')
  storage[h[1]] = links_to_scrape  	   
scrapeWikiArticle("https://en.wikipedia.org/wiki/Mathematics")
for i in queue:
    if len(i.split("/")) <= 3:
     count +=1
     if count <80:
        scrapeWikiArticle("https://en.wikipedia.org" + i) 
        queue.pop(0) 

print(len(queue))

storage

list(storage.keys())[0:10]

feature_vec

list(feature_vec.keys())[0:10]

# importing the wiki math article- sheet 1 given by anirban sir and creating a graph

import pandas as pd
from statistics import mode
#Import the CSV file and get it's link and label
file = pd.read_csv("Wiki Math Articles - Sheet1.csv")
file = file[['Link' , 'Label']]
file = file.dropna() #removing the missing values

rv =list(file['Link'])

#just for testing
s  = nx.Graph()
for i in storage:
  edges = storage[i]
  for edge in edges:
    s.add_edge(i,edge)

nx.set_node_attributes(s, feature_vec, "vector")
nx.set_node_attributes(s, key_w, "key_words")

s.nodes()
# the nodes will be the /wiki/title

rv =list(file['Link'])

rvl=list(file['Label'])

lit1=[]
for i in storage.values():
  for j in i:
    lit1.append(j)
lit = []
label1=[]
for i in range(len(rv)):
  if rv[i] in lit1:
    lit.append(rv[i]) 
    label1.append(rvl[i])

import pandas as pd
d={"Links":lit,"Labels":label1}
df = pd.DataFrame(d)
df.head(5)

df

import pandas as pd
from statistics import mode
#importing the output file sent by Anirban sir and extracting its labels anmd links
work=pd.read_csv("Output.csv")
work=work[['Link', 'Label']]
#removing empty nodes
work=work.dropna()

#There are multiple links with different labels so cleaning them by updating labels of links with most occuring label
for i in range(len(work)):
  labels=list()
  for j in range(len(work)):
    try:
      try:
        l1=work['Link'][i]
      except:
        pass
      try:
        l2=work['Link'][j]
      except:
        pass
      #if the labels are same
      if(l1==l2):
        try:
          labels.append(work['Label'][j])
        except:
          pass
    except ValueError:
      pass
  temp=labels
  cnt=0
  link_label=temp[0]
  for j in range(len(temp)):
    frequency=temp.count(temp[j])
    #replacing the labels with the most occuring label
    if(frequency>cnt):
      cnt=frequency
      link_label=temp[j]
  label=link_label
  for j in range(len(work)):
    try:
      l1=work['Link'][i]
    except:
      pass
    try:
      l2=work['Link'][j]
    except:
      pass
    #giving the most occuring label as the final label to that link
    if(l1==l2):
      try:
        work[j]['Link'] = label
      except:
        pass
#removing duplicates
work=work.drop_duplicates()

work

#assuming first 10 links as root nodes of the graph and appending it to the root list
n=10
root=list()
#difficulty level dictionary
level=dict()
for i in range(n):
  node=work['Link'][i]
  cur_label=work['Label'][i]
  level[node]=cur_label
  root.append(node)
print(root)

#returning the dictionary of difficulty level
level

#making wikipedia graph by connecting children links with their root links by visiting the root links
import requests
from bs4 import BeautifulSoup
l=list()
for i in range(len(root)):
  l.append(root[i])
#denoting the visited nodes as explored list
explored=list()
#parent node dictionary having keys as all the links and values as their parent/root links
parent_node=dict()
while(len(l)!=0):
  #creating a temporary root
  r=l.pop(0)
  #visiting the temporary root links and scraping it to extract all the links starting with wiki
  url=requests.get(r)
  html_page=BeautifulSoup(url.content, 'html.parser')
  urls=html_page.find_all('a')
  #children node list 
  children=list()
  for link in urls:
    t=link.get('href')
    #checking if the scraped links starts with /wiki/
    if t is not None and t.startswith('/wiki/') and ":" not in t:
      res='https://en.wikipedia.org' + t
      occur=False
      for j in range(len(children)):
        if(children[j]==res):
          occur=True
          break
      if(occur==False):
        #if the links of scraped url didin't get found then append it to the list of children nodes of that root
        children.append(res)
        if(len(children)==10):
          break
    for k in range(len(children)):
      #checking if the children nodes already visited or not. If not then append that link to visited/explored list 
      if(children[k] not in explored):
        explored.append(children[k])
        parent_node[children[k]]=r
  if(len(l)>1000):
    break

#creating the graph or the dictionary having parent_node as their keys and their values as their child node links
graph={}
for child in parent_node:
  p=parent_node[child]
  if(p not in graph):
    graph[p]=list()
  graph[p].append(child)
print(graph)

#creating the graph of the graph dictionary by visualizing it
import matplotlib.pyplot as plt
import networkx as nx
class GraphVisualization:
  def __init__(self):
    #edge_set is a list of all edge sets in graph
    self.edge_set=list()
  #function to add an edge between 2 vertices and append it to the edge_set list
  def add_edge(self, x, y):
    self.edge_set.append([x, y])
  #function to visualize the graph by adding edges from edge_set to the graph and drawing it using nx.draw_networkx
  def visualize(self):
    G = nx.Graph()
    G.add_edges_from(self.edge_set)
    nx.draw_networkx(G)
    plt.rcParams['figure.figsize']=[50, 50]
    plt.show()
  #function to extract the graph
  def extract_graph(self):
    G=nx.Graph()
    G.add_edges_from(self.edge_set)
    return G
Graph=GraphVisualization()

#adding edges between the nodes- parent and children
for i in graph:
  nodes=graph[i]
  for edge in nodes:
    Graph.add_edge(i,edge)
Graph.visualize()
#adding edges between the parent and their children nodes
g=nx.Graph()
for i in graph:
  nodes=graph[i]
  for edge in nodes:
    g.add_edge(i,edge)
#drawing the graph with their labels
nx.draw(g, with_labels=True)

#degree centrality metrics of the graph
nx.degree_centrality(g)

#betweenness centrality metrics of the graph
nx.betweenness_centrality(g)

#clustering coefficients of the nodes of the graph
nx.clustering(g)

pip install node2vec

#extracting node embeddings of graph by using node2vec
import networkx as nx
from node2vec import Node2Vec
#precomputing probabilities and generating random walks where walk_length is number of nodes in each walk, num_walks is number of walks per node and workers are the number of workers for parallel execution 
node2vec=Node2Vec(g, dimensions=64, walk_length=30, num_walks=200, workers=4)
#embedding nodes and creating a model
model = node2vec.fit(window=10, min_count=1, batch_words=4)
#extracting the node-embedding vectors of the graph
node_embeddings=model.wv.vectors
#saving node-embeddings and model in node_embeddings.emb and node_embeddings.model
model.wv.save_word2vec_format("node_embeddings.emb")
model.save("node_embeddings.model")

import numpy as np
print(node_embeddings)
np.shape(node_embeddings)